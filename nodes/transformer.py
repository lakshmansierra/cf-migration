import os
import json
from typing import Dict, Any
from gen_ai_hub.proxy.langchain.openai import ChatOpenAI
from langchain.schema import HumanMessage
    
from utils.file_ops import read_text_file, save_dict_to_file


# ------------------------
# SAP AI Core Credentials
# ------------------------
os.environ["AICORE_AUTH_URL"] = "https://gen-ai.authentication.us10.hana.ondemand.com/oauth/token"
os.environ["AICORE_CLIENT_ID"] = "sb-42a29a03-b2f4-47de-9a41-e0936be9aaf5!b256749|aicore!b164"
os.environ["AICORE_CLIENT_SECRET"] = "b5e6caee-15aa-493a-a6ac-1fef0ab6e9fe$Satg7UGYPLsz5YYeXefHpbwTfEqqCkQEbasMDPGHAgU="
os.environ["AICORE_RESOURCE_GROUP"] = "default"
os.environ["AICORE_BASE_URL"] = "https://api.ai.prod.us-east-1.aws.ml.hana.ondemand.com/v2"

# --- Model Deployment ---
LLM_DEPLOYMENT_ID = "dadede28a723f679"

# ------------------------
# System prompt for transformer
# ------------------------
SYSTEM_PROMPT = """
You are a migration transformer responsible for converting files from SAP Neo projects into their
corresponding Cloud Foundry (CF) equivalents, using only the migration plan generated by planner.py.

You will receive a JSON object that contains:
{
  "plan": [
    {
      "file": "<original Neo relative path>",
      "reason": "<why this file exists and how it should migrate>",
      "action": "<auto-decided label such as 'transform', 'adapt', 'copy', 'ignore', 'manual_review'>",
      "snippets": "<trimmed content sample>",
      "target": "<auto-inferred Cloud Foundry target path>"
    }
  ],
  "file_content": "<the full text content of the file>"
}

---

### üéØ Your Goal
- Use the migration plan details and `file_content` to automatically produce the transformed CF-compatible file.
- Follow the intent described by the `reason`, `action`, and `target` fields.
- You do **not** need to follow fixed migration templates ‚Äî infer what is appropriate from the data itself.
- Use the `target` field to decide the output file location, unless a better structure is clearly implied by context.

---

### üì¶ Expected Output
Return a **single valid JSON object only**, structured as:

{
  "target_path": "<final relative path where the converted file should be stored>",
  "converted_content": "<string - new file content>",
  "encoding": "utf-8",
  "notes": "<optional - reasoning or manual steps>",
  "error": "<optional - only if something prevents conversion>"
}

---

### üß† Transformation Rules
1. Always preserve the **intent and functionality** of the original file.
2. If uncertain about how to transform, provide an `"error"` with a brief explanation.
3. The output **must be valid JSON only** ‚Äî no commentary or markdown.

---

### ‚öôÔ∏è Additional Rule: Add Dependencies Based on Project Purpose
- Analyze the `reason`, `snippets`, and `file_content` to **infer the project‚Äôs purpose**  
  (e.g., UI app, Java backend, Node.js service, MTA module, etc.).
- Based on that analysis, **create or modify a dependency descriptor file** if it is missing or incomplete.
- The dependency descriptor should match the project type and use an appropriate format:
  - **Node.js apps:** `package.json`
  - **Java projects:** `pom.xml`
  - **Python apps:** `requirements.txt`
  - **MTA or multi-module projects:** `mta.yaml`
- If such a file already exists, update it logically rather than duplicating.
- If a new one must be created, generate it under a relevant name and extension (e.g., `.json`, `.yaml`, `.xml`).

---

### üßæ Example (for understanding only)
If a `snippets` section contains Node.js imports or an `express` server:
‚Üí Generate or update a `package.json` file with necessary dependencies and scripts.

If `file_content` references Maven, Spring Boot, or Java packages:
‚Üí Generate or update a `pom.xml` with matching dependencies and build plugins.

---

### üö¶ Output Reminder
Return only **one JSON object** ‚Äî no extra explanation or text outside the JSON.
"""

# ------------------------
# Initialize LLM
# ------------------------
llm = ChatOpenAI(deployment_id=LLM_DEPLOYMENT_ID, temperature=0)


# ------------------------
# Transform files
# ------------------------
def transform_files(repo_root: str, plan: Dict[str, Any]) -> Dict[str, str]:
    """
    Transform files according to the migration plan.
    - Prints all transformer responses to terminal.
    - Skips ignored/manual files.
    - Saves output as transformer_output.json in CWD.
    """
    results: Dict[str, str] = {}
    items = plan.get("plan", [])

    print(f"\nüöÄ Starting transformation of {len(items)} files...")

    for i, item in enumerate(items, start=1):
        rel = item.get("file")
        target = item.get("target") or rel
        action = (item.get("action") or "").lower()

        # Skip ignored/manual_review actions
        if action in {"ignore", "manual_review"}:
            print(f"‚è≠Ô∏è  [{i}] Skipping '{rel}' (action: {action})")
            continue

        src_path = os.path.join(repo_root, rel)
        if not os.path.exists(src_path):
            print(f"‚ö†Ô∏è  [{i}] Missing source file: {rel}")
            results[target] = f"# MISSING SOURCE: {rel}\n"
            continue

        content = read_text_file(src_path)
        payload = {
            "plan_item": item,
            "file_content": content,
            "instructions": SYSTEM_PROMPT
        }

        prompt = json.dumps(payload, indent=2)

        print(f"\nüß† [{i}] Transforming: {rel} ‚Üí {target}")

        try:
            resp = llm.invoke([HumanMessage(content=prompt)])
            raw_content = getattr(resp, "content", str(resp))
            print(f"\nüì• Raw LLM response for {rel}:\n{raw_content}\n")

            # Attempt to extract JSON from possibly wrapped output
            start = raw_content.find("{")
            end = raw_content.rfind("}") + 1
            json_str = raw_content[start:end] if start != -1 and end > start else "{}"

            parsed = json.loads(json_str)
            converted = parsed.get("converted_content", "").strip()

            if converted:
                results[target] = converted
                print(f"‚úÖ [{i}] Transformed successfully.")
            else:
                results[target] = content  # fallback
                print(f"‚ö†Ô∏è [{i}] Transformer returned empty content, using original file.")

        except Exception as e:
            results[target] = f"# LLM FAILED: {str(e)}\n"
            print(f"‚ùå [{i}] Transformer failed for {rel}: {e}")

    # Save all results to transformer_output.json
    output_path = os.path.join(os.getcwd(), "transformer_output.json")
    save_dict_to_file(results, output_path)
    print(f"\nüìÑ Transformer output saved to: {output_path}")

    return results



