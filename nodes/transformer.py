import os
import json
from typing import Dict, Any
from gen_ai_hub.proxy.langchain.openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from nodes.planner import llm
from nodes.planner import LLM_DEPLOYMENT_ID
   
from utils.file_ops import read_text_file, save_dict_to_file
 
 
 
# ------------------------
# System prompt for transformer
# ------------------------
SYSTEM_PROMPT = """
You are a migration transformer responsible for converting files from SAP Neo projects into their
corresponding Cloud Foundry (CF) equivalents, using only the migration plan generated by planner.py.
 
You will receive a JSON object that contains:
{
  "plan": [
    {
      "file": "<original Neo relative path>",
      "reason": "<why this file exists and how it should migrate>",
      "action": "<auto-decided label such as 'transform', 'adapt', 'copy', 'ignore', 'manual_review'>",
      "snippets": "<trimmed content sample>",
      "target": "<auto-inferred Cloud Foundry target path>"
    }
  ],
  "file_content": "<the full text content of the file>"
}
 
---
 
### Your goal:
- Use the plan details and file content to produce the transformed file content automatically.
- Follow the intent described in the plan (`reason` + `action` + `target`).
- You do not need to follow predefined migration rules; instead, infer what‚Äôs appropriate from the data itself.
- Decide the final directory or file structure using the 'target' field, unless a better location is clear from context.
 
---
 
### Additional Rule: Add Dependencies Based on Project Purpose
 
- Analyze the `reason`, `snippets`, and `file_content` to infer the project's purpose.
- Based on that, include or modify a dependency descriptor file .
 
You have to create such files under the name in relevant with extension
 
For example: .json, yaml, xml etc
 
 
 
- Always merge inferred dependencies without overwriting existing ones.
- Only add what‚Äôs essential for Cloud Foundry runtime compatibility
 
 
 
### Output Format:
Return a **single valid JSON object only**, with this structure:
 
```json
{
  "target_path": "<final relative path where the converted file should be stored>",
  "converted_content": "<string - new file content>",
  "encoding": "utf-8",
  "notes": "<optional - reasoning or manual follow-up steps>",
  "error": "<optional - only if something prevents conversion>"
}
```
 
---
 
### Guidelines:
- Always preserve the functionality and intent of the original file.
- Ensure added dependencies match the detected project type.
- If unsure of what dependencies to add or if the project type cannot be inferred, return an `error` field with a short note.
- Output must be **val
"""
 
 
 
# ------------------------
# Initialize LLM
# ------------------------
llm = ChatOpenAI(deployment_id=LLM_DEPLOYMENT_ID, temperature=0.2)
 
 
# ------------------------
# Transform files
# ------------------------
def transform_files(repo_root: str, plan: Dict[str, Any]) -> Dict[str, str]:
    """
    Transform files according to the migration plan.
    - Prints all transformer responses to terminal.
    - Skips ignored/manual files.
    - Saves output as transformer_output.json in CWD.
    """
    results: Dict[str, str] = {}
    items = plan.get("plan", [])
 
    print(f"\nüöÄ Starting transformation of {len(items)} files...")
 
    for i, item in enumerate(items, start=1):
        rel = item.get("file")
        target = item.get("target") or rel
        action = (item.get("action") or "").lower()
 
        # Skip ignored/manual_review actions
        if action in {"ignore", "manual_review"}:
            print(f"‚è≠Ô∏è  [{i}] Skipping '{rel}' (action: {action})")
            continue
 
        src_path = os.path.join(repo_root, rel)
        if not os.path.exists(src_path):
            print(f"‚ö†Ô∏è  [{i}] Missing source file: {rel}")
            results[target] = f"# MISSING SOURCE: {rel}\n"
            continue
 
        content = read_text_file(src_path)
        payload = {
            "plan_item": item,
            "file_content": content,
            "instructions": SYSTEM_PROMPT
        }
 
        prompt = json.dumps(payload, indent=2)
 
        print(f"\nüß† [{i}] Transforming: {rel} ‚Üí {target}")
 
        try:
            resp = llm.invoke([HumanMessage(content=prompt)])
            raw_content = getattr(resp, "content", str(resp))
            print(f"\nüì• Raw LLM response for {rel}:\n{raw_content}\n")
 
            # Attempt to extract JSON from possibly wrapped output
            start = raw_content.find("{")
            end = raw_content.rfind("}") + 1
            json_str = raw_content[start:end] if start != -1 and end > start else "{}"
 
            parsed = json.loads(json_str)
            converted = parsed.get("converted_content", "").strip()
 
            if converted:
                results[target] = converted
                print(f"‚úÖ [{i}] Transformed successfully.")
            else:
                results[target] = content  # fallback
                print(f"‚ö†Ô∏è [{i}] Transformer returned empty content, using original file.")
 
        except Exception as e:
            results[target] = f"# LLM FAILED: {str(e)}\n"
            print(f"‚ùå [{i}] Transformer failed for {rel}: {e}")
 
    # Save all results to transformer_output.json
    output_path = os.path.join(os.getcwd(), "transformer_output.json")
    save_dict_to_file(results, output_path)
    print(f"\nüìÑ Transformer output saved to: {output_path}")
 
    return results
 